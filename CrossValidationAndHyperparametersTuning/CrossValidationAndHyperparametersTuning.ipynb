{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Cross-Validation**\n",
        "# What is Cross-Validation?\n",
        "Cross-validation is a statistical method used to evaluate how well a machine learning model generalizes to an independent dataset. The data is split into multiple parts, with each part being used as a test set while the remaining data is used for training.\n",
        "\n",
        "**Process:**\n",
        "Split the Data: Divide the data into multiple parts (folds).\n",
        "Training and Testing: For each fold, use one part as test data and the rest as training data.\n",
        "Generalization: Cross-validation helps to reduce overfitting and ensures that the model’s accuracy is more generalized across unseen data.\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "WgILl2A1Zrb8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "McbN6NPG_9_K",
        "outputId": "7bb9db19-cee6-477a-b53f-42470fee63a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-validation scores: [0.96666667 1.         0.93333333 0.96666667 1.        ]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "\n",
        "\n",
        "# Apply 5-fold cross-validation\n",
        "\n",
        "scores = cross_val_score(model, X, y, cv=5)\n",
        "\n",
        "print(f\"Cross-validation scores: {scores}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning\n",
        "**What are Hyperparameters?**\n",
        "Hyperparameters are parameters that are set before the learning process begins, unlike model parameters that are learned from the data. Hyperparameters affect the model’s performance and generalization ability. Examples include:\n",
        "\n",
        "Learning rate\n",
        "Batch size\n",
        "Number of neighbors (in KNN)\n",
        "Number of trees (in Random Forest)"
      ],
      "metadata": {
        "id": "KeoVsBxgaOjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methods of Hyperparameter Tuning:\n",
        "**Manual Tuning:** Adjusting hyperparameters one by one and evaluating performance. This can be time-consuming but is useful for understanding the effect of each parameter.\n",
        "                            **Grid Search:** A technique where you define a parameter grid and evaluate every possible combination. It’s computationally expensive but thorough.\n",
        "**Randomized Search:** Instead of evaluating all combinations, random search evaluates a random sample of combinations. This is more computationally efficient than grid search."
      ],
      "metadata": {
        "id": "NWFo0h0EaYYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "\n",
        "# Define model and parameters\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "param_grid = {'n_estimators': [10, 50, 100], 'max_depth': [None, 10, 20]}\n",
        "\n",
        "\n",
        "\n",
        "# Apply Grid Search\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "print(\"Best parameters found:\", grid_search.best_params_)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYKZ_hMTaVBA",
        "outputId": "40d1502b-543f-4bd3-ee53-789ba3145bf6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found: {'max_depth': 10, 'n_estimators': 50}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from scipy.stats import randint\n",
        "\n",
        "\n",
        "\n",
        "# Define model and parameters\n",
        "\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "param_dist = {'n_estimators': randint(10, 100), 'max_depth': randint(1, 20)}\n",
        "\n",
        "\n",
        "\n",
        "# Apply Randomized Search\n",
        "\n",
        "random_search = RandomizedSearchCV(model, param_dist, n_iter=100, cv=5)\n",
        "\n",
        "random_search.fit(X, y)\n",
        "\n",
        "print(\"Best parameters found:\", random_search.best_params_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDW2sCO1as_i",
        "outputId": "f2988158-4653-4da5-a0a1-b10169cb7133"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found: {'max_depth': 11, 'n_estimators': 63}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Thoughts**\n",
        "Cross-validation and hyperparameter tuning are indispensable techniques in machine learning. Cross-validation helps ensure the model generalizes well to unseen data, while hyperparameter tuning optimizes the model’s performance. By applying these methods systematically, you can significantly improve your model’s accuracy, robustness, and reliability in real-world applications.\n",
        "\n",
        "This Lecture provides a comprehensive guide on implementing cross-validation and hyperparameter tuning in machine learning projects, along with practical examples and challenges associated with these techniques."
      ],
      "metadata": {
        "id": "cipqeWQaa1pd"
      }
    }
  ]
}